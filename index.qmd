---
title: Genomics Data Mining
---

## Expectations

TECHNNICAL

We expect a basic understanding of programming (we use Python, but any programming language will do).
Only functions that are created within this project will have documentation, otherwise you are expected to be able to read and understand what they are doing through their own official documentation.

BIOLOGY

We are only looking at human biology, and expect you to know at least the following: 

- Genome
  - Think of this as an entire library of all the pieces and instructions for a human
  - The collection of all chromosomes
- Chromosome
  - Think of this as the organization method of the library
  - One single DNA molecule, packaged tightly
- Gene
  - Think of this as individual recipes that make something, but just the ingredients list
  - A section of DNA (or a section of RNA) that does something
- DNA
  - Think of this as the letters that are used for writing everything
  - Made up of four chemicals
  - Is a double-helix (i.e., two-strands)
  - DNA is "the original" plans, so to keep them safe, copies are made; Transcription
  - Copies are then used to do what the plans say
- RNA
  - Think of this as the copy ??? Not sure what to put here
  - RNA is one-side of the DNA strand
  - It is how instructions can be carried out; Instructions include "make a protein"; Translation 
- mRNA
- Protein
  - Proteins are 


- Humans are essentially meat-bags
- We have different kinds of "meat" but they're all made up of cells
- The types of cells and types of meats are "Cellular Biology" - we're not going to focus on this, at all
- We are going to be looking at things that are smaller than cells, and basically how they impact cells (which then impact other things)
- 



## Setup

```{ojs}
//| label: section-div-01
//| column: screen
//| echo: False

html`
<div class="section-div">
  <div class="current">Setup</div>
  <div>Data Acquisition</div>
  <div>Data Understanding</div>
  <div>Data Cleaning</div>
  <div>Dimension Reduction</div>
  <div>Statistical Modelling</div>
  <div>Conclusions</div>
</div>
`
 
```

Establishing a reproducible analysis environment.

```{python}
#| label: analyze-setup-debugger
#| eval: false
#| output: true

# Used if needed to debut environment issues

import sys, os, site
print("exe:", sys.executable)
print("ver:", sys.version)
print("prefix:", sys.prefix)
print("base_prefix:", sys.base_prefix)
print("cwd:", os.getcwd())
print("usersite:", site.getusersitepackages())


for k in ["PYTHONHOME", "PYTHONPATH", "CONDA_PREFIX", "VIRTUAL_ENV"]:
    print(k, "=", os.environ.get(k))


import os, shutil, subprocess
from pathlib import Path

print("sys.executable:", sys.executable)
print("CONDA_PREFIX:", os.getenv("CONDA_PREFIX"))
print("CONDA_PREFIX_1:", os.getenv("CONDA_PREFIX_1"))
print("CONDA_EXE:", os.getenv("CONDA_EXE"))
print("which conda:", shutil.which("conda"))

# Try common locations
candidates = []
if os.getenv("CONDA_EXE"):
    candidates.append(os.getenv("CONDA_EXE"))
if os.getenv("CONDA_PREFIX_1"):
    candidates.append(str(Path(os.getenv("CONDA_PREFIX_1")) / "Scripts" / "conda.exe"))
if os.getenv("CONDA_ROOT"):
    candidates.append(str(Path(os.getenv("CONDA_ROOT")) / "Scripts" / "conda.exe"))

print("\nCandidates:")
for c in candidates:
    print(" ", c, "exists=", Path(c).exists())

# Try running conda directly if we found one
conda = next((c for c in candidates if Path(c).exists()), None) or shutil.which("conda")
print("\nResolved conda:", conda)

if conda:
    p = subprocess.run([conda, "--version"], capture_output=True, text=True)
    print("conda --version rc:", p.returncode)
    print("stdout:", p.stdout.strip())
    print("stderr:", p.stderr.strip())
else:
    print("No conda found in this process.")



```

### Package Imports

```{python}
#| label: analyze-setup-packages
#| eval: true
#| output: false

# --- Standard library ---
import importlib.metadata
import importlib
import json
import os
import platform
import random
import re
import shutil
import subprocess
import sys
import tarfile
from pathlib import Path
from urllib.parse import urlsplit

# --- Third-party ---
import itables
import numpy as np
import pandas as pd
import requests
import sklearn
import yaml
from IPython.display import Markdown, display
from ydata_profiling import ProfileReport

# --- Currently Testing ---
import time
import cbioportal
from cbioportal.rest import ApiException
from pprint import pprint

# --- Local / project ---
import config

importlib.reload(config)
```

### Custom Objects
```{python}
#| label: analyze-setup-custom-objects
#| eval: true
#| output: false

from dataclasses import dataclass, field
from typing import Optional, List, Dict

@dataclass
class CancerStudy:
    # Core identity
    study_id: str
    study_name: str
    description: Optional[str]

    # Cancer classification
    cancer_type_id: Optional[str]
    cancer_type_name: Optional[str]

    # Clinical summary
    sample_count: Optional[int] = None
    patient_count: Optional[int] = None

    # Data modalities present
    data_types: List[str] = field(default_factory=list)

    # Provenance
    reference: Optional[str] = None
    citation: Optional[str] = None
    pmid: Optional[str] = None

    # Internal / system metadata
    source_url: Optional[str] = None
    raw_meta: Dict[str, str] = field(default_factory=dict)

```

### Parameters

:::{.callout-important}

Most parameters are defined in the config.py file, and will eventually be part of environment variables

:::

```{python}
#| label: analyze-setup-parameters
#| eval: true
#| output: true

# Parameters
RANDOM_SEED = 3660
```

### Package Defaults / Options

```{python}
#| label: analyze-setup-package-options
#| eval: true
#| output: false

# ITables
# itables.init_notebook_mode()

# Pandas
pd.set_option("display.max_rows", 30)
pd.set_option("display.max_columns", 50)
pd.set_option("display.width", 120)
pd.set_option("display.max_colwidth", 60)

# Randomness
os.environ["PYTHONHASHSEED"] = str(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
# Note for later:
# - use random_state=RANDOM_SEED in sklearn models
# - TSNE(random_state=RANDOM_SEED)
# - train_test_split(..., random_state=RANDOM_SEED)

# Others 
# ...
```

### Helper Functions

```{python}
#| label: analyze-setup-helper-functions
#| eval: true
#| output: false

def download_and_extract_studies(*study_ids: str) -> list[Path]:
    return [_download_and_extract_study(study) for study in study_ids]

def _download_and_extract_study(study_id: str):
    url = f"{config.DATAHUB_BASE}/{study_id}.tar.gz"
    dataset_file = download_dataset(url)
    dataset_path = extract_dataset(dataset_file)
    return dataset_path

def download_dataset(url, /, *, folder: Path = config.DEFAULT_DATASET_DOWNLOAD_FOLDER, force: bool = False) -> Path:
    folder.mkdir(parents=True, exist_ok=True)
    
    filename = Path(urlsplit(url).path).name
    target = folder / filename

    if force or not target.exists():
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        with target.open("wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
    return target

def extract_dataset(file: Path, /, *, folder: Path = config.DEFAULT_DATASET_EXTRACT_FOLDER, force: bool = False) -> Path:
    base_name = file.name.removesuffix(".tar.gz")
    extract_path = folder / base_name
    
    folder.mkdir(parents=True, exist_ok=True)

    if force or not extract_path.exists():
        with tarfile.open(file, "r:gz") as tar:
            tar.extractall(folder)

    return extract_path

```

### Environment

Checks

```{python}
#| label: analyze-setup-environment
#| eval: true
#| display: false

# ---------- helpers ----------

def _run(cmd: list[str]) -> tuple[int, str, str]:
    p = subprocess.run(cmd, capture_output=True, text=True)
    return p.returncode, p.stdout, p.stderr

def _canonical(name: str) -> str:
    return re.sub(r"[-_.]+", "-", name).lower().strip()

def read_environment_yml(path: str | Path = "environment.yml"):
    """
    Parse environment.yml into:
      - conda_chosen: conda deps declared in environment.yml (name -> version/constraint/None)
      - pip_chosen: pip deps declared under `- pip:` in environment.yml (name -> version/constraint/None)
    Notes:
      - Handles conda pins like `name`, `name=ver`, `name=ver=build`
      - Handles constraints like `python>=3.11`, `numpy<2`
      - Normalizes accidental leading '=' in version strings (e.g., '=1.2.3')
    """
    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}

    conda_chosen: dict[str, str | None] = {}
    pip_chosen: dict[str, str | None] = {}

    deps = data.get("dependencies", []) or []
    for dep in deps:
        if isinstance(dep, str):
            s = dep.strip()
            if not s:
                continue

            # constraint case: python>=3.11, numpy<2, etc.
            m = re.match(r"^([A-Za-z0-9_.-]+)\s*([<>=!~].+)$", s)
            if m:
                conda_chosen[_canonical(m.group(1))] = m.group(2).strip()
                continue

            # conda pins can be: name, name=ver, name=ver=build
            parts = s.split("=")
            name = _canonical(parts[0])
            ver = parts[1] if len(parts) >= 2 else None
            if ver is not None:
                ver = ver.strip()
                if ver.startswith("=") and not ver.startswith("=="):
                    ver = ver.lstrip("=")
            conda_chosen[name] = ver

        elif isinstance(dep, dict) and "pip" in dep:
            for p in dep.get("pip", []) or []:
                ps = str(p).strip()
                if not ps:
                    continue
                if "==" in ps:
                    pkg, ver = ps.split("==", 1)
                    pip_chosen[_canonical(pkg)] = ver.strip()
                else:
                    # store non-exact constraint as-is (>=, <=, ~=, etc.)
                    pkg = re.split(r"[<>=!~]", ps, maxsplit=1)[0].strip()
                    constraint = ps[len(pkg):].strip() or None
                    pip_chosen[_canonical(pkg)] = constraint

    return data, conda_chosen, pip_chosen


def _conda_cmd() -> list[str] | None:
    """
    Find conda.exe even when it's not on PATH (common for notebooks on Windows).

    Priority:
      1) CONDA_EXE env var (if set)
      2) PATH lookup (shutil.which("conda"))
      3) Derive from CONDA_PREFIX (Windows Anaconda layout):
         CONDA_PREFIX = ...\\anaconda3\\envs\\<env>
         conda.exe    = ...\\anaconda3\\Scripts\\conda.exe
    """
    # 1) explicit env var (often not set in notebooks)
    conda_exe = os.getenv("CONDA_EXE")
    if conda_exe and Path(conda_exe).exists():
        return [conda_exe]

    # 2) PATH lookup
    which = shutil.which("conda")
    if which:
        return [which]

    # 3) derive from CONDA_PREFIX on Windows
    conda_prefix = os.getenv("CONDA_PREFIX")
    if conda_prefix:
        p = Path(conda_prefix)
        # Typical: .../anaconda3/envs/<env> -> base is parent of "envs"
        if p.parent.name.lower() == "envs":
            base = p.parents[1]
        else:
            # Sometimes CONDA_PREFIX might be the base env itself
            base = p
        candidate = base / "Scripts" / "conda.exe"
        if candidate.exists():
            return [str(candidate)]

    return None


def conda_available() -> bool:
    cmd = _conda_cmd()
    if not cmd:
        return False
    try:
        subprocess.run(cmd + ["--version"], capture_output=True, text=True, check=True)
        return True
    except Exception:
        return False


def get_conda_installed_versions_strict() -> dict[str, str]:
    cmd = _conda_cmd()
    if not cmd:
        raise FileNotFoundError("Could not find conda executable...")

    prefix = os.getenv("CONDA_PREFIX")
    if not prefix:
        raise RuntimeError("CONDA_PREFIX is not set; cannot target env.")

    proc = subprocess.run(
        cmd + ["list", "--json", "--prefix", prefix],
        capture_output=True,
        text=True,
    )
    if proc.returncode != 0:
        raise RuntimeError(f"conda list failed:\nSTDOUT:\n{proc.stdout}\nSTDERR:\n{proc.stderr}")

    items = json.loads(proc.stdout)
    out = {}
    for it in items:
        name = (it.get("name") or "").strip().lower()
        ver = (it.get("version") or "").strip()
        if name and ver:
            out[name] = ver

    out["python"] = sys.version.split()[0]
    return out

def get_conda_chosen_from_history() -> dict[str, str | None]:
    """
    Packages explicitly requested in this env (best proxy):
    `conda env export --from-history [--prefix ...]`

    Returns: canonical_name -> constraint/pin/None
      - Handles pins:   name, name=1.2.3, name=1.2.3=build
      - Handles constraints: name<2, name>=3.11, name ~=1.4, etc.
    """
    cmd = _conda_cmd()
    if not cmd:
        return {}

    # Prefer CONDA_PREFIX if available; otherwise fall back to interpreter prefix
    prefix = os.getenv("CONDA_PREFIX") or str(Path(sys.executable).resolve().parent.parent)

    proc = subprocess.run(
        cmd + ["env", "export", "--from-history", "--prefix", prefix],
        capture_output=True,
        text=True,
    )
    if proc.returncode != 0:
        # If conda isn't available in this context, just return empty (don't blow up rendering)
        return {}

    data = yaml.safe_load(proc.stdout) or {}
    deps = data.get("dependencies", []) or []

    chosen: dict[str, str | None] = {}

    for dep in deps:
        if not isinstance(dep, str):
            continue
        s = dep.strip()
        if not s:
            continue

        # constraint case: python>=3.11, setuptools<80, numpy!=2.0, etc.
        m = re.match(r"^([A-Za-z0-9_.-]+)\s*([<>=!~].+)$", s)
        if m:
            chosen[_canonical(m.group(1))] = m.group(2).strip()
            continue

        # pin case: name, name=ver, name=ver=build
        parts = s.split("=")
        name = _canonical(parts[0])
        ver = parts[1].strip().lstrip("=") if len(parts) >= 2 else None
        chosen[name] = ver

    return chosen



def get_pip_installed_all() -> dict[str, str]:
    """
    pip list as json; best effort even inside conda env.
    """
    rc, out, err = _run([sys.executable, "-m", "pip", "list", "--format=json"])
    if rc != 0:
        return {}
    items = json.loads(out)
    return {_canonical(it["name"]): it["version"] for it in items}


def version_status(expected: str | None, installed: str | None) -> str:
    if installed is None:
        return "MISSING"
    if expected is None:
        return "OK (un-pinned)"

    expected = expected.strip()

    # Treat "=1.2.3" as an exact pin (common accidental format)
    if expected.startswith("=") and not expected.startswith("=="):
        expected = expected.lstrip("=")

    # True constraints (>=, <=, ==, ~=, etc.)
    if expected and expected[0] in "<>!~" or expected.startswith("=="):
        return "CHECK (constraint)"

    if expected.endswith(".*"):
        return "OK" if installed.startswith(expected[:-1]) else "MISMATCH"

    return "OK" if installed == expected else "MISMATCH"



# ---------- run ----------

env_data, yml_conda_chosen, yml_pip_chosen = read_environment_yml("environment.yml")

# Capture interpreter details (your "custom interpreter" concern)
interpreter_rows = [
    {"Key": "sys.executable", "Value": sys.executable},
    {"Key": "sys.version", "Value": sys.version},
]
if os.getenv("CONDA_PREFIX"):
    interpreter_rows.append({"Key": "CONDA_PREFIX", "Value": os.getenv("CONDA_PREFIX")})
df_interpreter = pd.DataFrame(interpreter_rows)
display(df_interpreter)

# Get actuals
try:
    conda_installed = get_conda_installed_versions_strict() if conda_available() else {}
except Exception as e:
    print("‚ö†Ô∏è Could not read conda installed packages:", e)
    conda_installed = {}

conda_history_chosen = get_conda_chosen_from_history() if conda_available() else {}
pip_installed = get_pip_installed_all()

rows: list[dict] = []

def add_manager_rows(
    manager: str,
    chosen_map: dict[str, str | None],
    installed_versions: dict[str, str],
):
    chosen_set = set(chosen_map.keys())

    # chosen packages (source of truth = environment.yml)
    for pkg in sorted(chosen_map.keys()):
        exp = chosen_map[pkg]
        got = installed_versions.get(pkg)
        status = version_status(exp, got)
        rows.append({
            "Manager": manager,
            "Package": pkg,
            "Chosen?": True,
            "Expected (environment.yml)": exp if exp is not None else "(un-pinned)",
            "Installed": got if got is not None else "(not installed)",
            "Status": status,
        })

    # dependencies (installed but not chosen)
    for pkg in sorted(set(installed_versions.keys()) - chosen_set):
        got = installed_versions.get(pkg)
        rows.append({
            "Manager": manager,
            "Package": pkg,
            "Chosen?": False,
            "Expected (environment.yml)": "(dependency)",
            "Installed": got,
            "Status": "INFO (dependency)",
        })

# Conda & Pip rows
add_manager_rows("conda", yml_conda_chosen, conda_installed)
add_manager_rows("pip", yml_pip_chosen, pip_installed)

df = pd.DataFrame(rows)

# Helpful sorting: chosen first, then manager, then package
df = df.sort_values(by=["Chosen?", "Manager", "Package"], ascending=[False, True, True]).reset_index(drop=True)
display(df)

# Summary counts for quick signal
summary = df.groupby(["Manager", "Status"]).size().reset_index(name="Count")
display(summary)

# Extra: show "explicit conda-history packages missing from environment.yml"
extra_explicit = []
if conda_history_chosen:
    for pkg in sorted(set(conda_history_chosen.keys()) - set(yml_conda_chosen.keys())):
        extra_explicit.append({
            "Package": pkg,
            "From conda history": conda_history_chosen[pkg],
            "In environment.yml?": "NO",
        })

df_extra_explicit = pd.DataFrame(extra_explicit)
if not df_extra_explicit.empty:
    print("‚ö†Ô∏è Packages explicitly requested in this conda env (history) but missing from environment.yml:")
    display(df_extra_explicit)
else:
    print("‚úÖ No extra explicit conda-history packages missing from environment.yml (or conda history unavailable).")


```

## Data Acquisition

```{ojs}
//| label: section-div-02
//| column: screen
//| echo: False

html`
<div class="section-div">
  <div>Setup</div>
  <div class="current">Data Acquisition</div>
  <div>Data Understanding</div>
  <div>Data Cleaning</div>
  <div>Dimension Reduction</div>
  <div>Statistical Modelling</div>
  <div>Conclusions</div>
</div>
`
 
```

We use the cBioPortal to gather data about what is available. 

### cBioPortal

#### Cancer Types
```{python}
#| label: analyze-data-acquisition-cancer-types-api
#| eval: true
#| display: true

# Cancer Types API
cancer_types_api_config = cbioportal.Configuration()
cancer_types_api_instance = cbioportal.CancerTypesApi(cbioportal.ApiClient(cancer_types_api_config))

cbioportal_cancer_types = cancer_types_api_instance.get_all_cancer_types_using_get()
cbioportal_cancer_types = pd.DataFrame([item.to_dict() for item in cbioportal_cancer_types])

itables.show(
    cbioportal_cancer_types
)

```

#### Studies
```{python}
#| label: analyze-data-acquisition-studies-api
#| eval: true
#| display: true
# Studies API
studies_api_instance = cbioportal.StudiesApi()

cbioportal_studies = studies_api_instance.get_all_studies_using_get()
cbioportal_studies = pd.DataFrame([item.to_dict() for item in cbioportal_studies])

itables.show(
    cbioportal_studies
)
```

#### Clinical Attributes
```{python}
#| label: analyze-data-acquisition-clinical-attributes-api
#| eval: true
#| display: true

# Clinical Attributes API

lgg_clinical_attributes = cbioportal.ClinicalAttributesApi().get_all_clinical_attributes_in_study_using_get('lgg_tcga_pan_can_atlas_2018')
lgg_clinical_attributes = pd.DataFrame([item.to_dict() for item in lgg_clinical_attributes])

itables.show(
    lgg_clinical_attributes
)

```

### Download and Extract

The Brain Lower Grade Glioma (TCGA, PanCancer Atlas) dataset was obtained from the cBioPortal for Cancer Genomics. The dataset was downloaded as a compressed tarball containing clinical, molecular, and metadata files.

**Dataset**: Brain Lower Grade Glioma (TCGA, PanCancer Atlas)  
**Source**: https://www.cbioportal.org/  
**File**: `lgg_tcga_pan_can_atlas_2018.tar.gz`

```{python}
#| label: analyze-data-acquisition-extract-package
#| eval: true
#| display: true

study_paths = download_and_extract_studies(
    "lgg_tcga_pan_can_atlas_2018",
    "lgg_ucsf_2014"
)
study_paths

```

::: callout-note
## cBioPortal Downloading

There is extensive documentation for how to download from cBioPortal. This includes manually through the browser, or with an API. https://docs.cbioportal.org/downloads/

:::

### Data Loading

Using information from the cBioPortal Summary tab and the downloaded data files, the following provides a high-level overview of the dataset, including patient counts, molecular data availability, mutation frequencies, and survival information.

```{python}
#| label: analyze-data-acquisition-loading-helper
#| eval: true
#| display: false



def describe_dataframe_table(df: pd.DataFrame) -> pd.DataFrame:
    rows = []

    for col in df.columns:
        s = df[col]

        row = {
            "Column Name": col,
            "Column Type": str(s.dtype),
            "Non-Null Count": s.notna().sum(),
            "Missing Count": s.isna().sum(),
        }

        if pd.api.types.is_numeric_dtype(s):
            row.update({
                "Data Kind": "Numeric",
                "Min": s.min(),
                "Max": s.max(),
                "Categories": None,
            })
        else:
            row.update({
                "Data Kind": "Categorical",
                "Min": None,
                "Max": None,
                "Categories": ", ".join(map(str, s.dropna().unique()[:10])),
            })

        rows.append(row)

    return pd.DataFrame(rows)
```

:::: column-page
::: panel-tabset
## Patient

```{python}
#| label: analyze-data-acquisition-loading-patient
#| eval: true
#| display: true


clinical_patient_file = Path("data/external/extracted/lgg_tcga_pan_can_atlas_2018/data_clinical_patient.txt")

df_clinical_patients = pd.read_csv(clinical_patient_file, sep="\t", comment="#")

described_df_clinical_patients = describe_dataframe_table(df_clinical_patients)


itables.show(
    described_df_clinical_patients,
    paging=False,
    text_in_header_can_be_selected=False,
    columnControl=["order", "colVisDropdown", "searchDropdown"],
    ordering={"indicators": False, "handler": False}
)

```

## Clinical Sample

```{python}
#| label: analyze-data-acquisition-loading-clinical-sample
#| eval: true
#| display: true


clinical_sample_file = Path("data/external/extracted/lgg_tcga_pan_can_atlas_2018/data_clinical_patient.txt")



df_clinical_sample = pd.read_csv(clinical_sample_file, sep="\t", comment="#")

```

## MRNA Seq




```{python}
#| label: analyze-data-acquisition-loading-mrna
#| eval: true
#| display: true

mrna_seq_file = Path("data/external/extracted/lgg_tcga_pan_can_atlas_2018/data_mrna_seq_v2_rsem.txt")


df_mrna_seq = pd.read_csv(mrna_seq_file, sep="\t", comment="#")

itables.show(
    df_mrna_seq
)

# expr = df.iloc[:, 2:]

# gene_summary = pd.DataFrame({
#     "mean": expr.mean(axis=1),
#     "median": expr.median(axis=1),
#     "std": expr.std(axis=1),
#     "variance": expr.var(axis=1),
#     "nonzero_fraction": (expr > 0).mean(axis=1)
# })

# gene_summary.head()







```

:::
::::

### TODO: data_clinical_sample.txt

### TODO: Summarize the 3 df's

### TODO:

Thesse are some words in this section



## Data Understanding

```{ojs}
//| label: section-div-03
//| column: screen
//| echo: False

html`
<div class="section-div">
  <div>Setup</div>
  <div>Data Acquisition</div>
  <div class="current">Data Understanding</div>
  <div>Data Cleaning</div>
  <div>Dimension Reduction</div>
  <div>Statistical Modelling</div>
  <div>Conclusions</div>
</div>
`
 
```

### Review
DNA has many Genes (i.e., regions)
A gene is a part of the DNA that does something, and is defined by specific coordinates (one start, and one end)
In order for those recipes to do anything, the DNA is copied to RNA, in a process called transcription.
This RNA is called "Pre-mRNA (Precursor mRNA or heterogenous nuclear RNA: hnRNA)"
The Pre-mRNA is processed by removing certain parts (slicing), adding stuff to one end (capping), and adding other stuff to the other end (polyadenylation), which then makes it "Mature mRNA" (we'll just call this mRNA from now). 
... in cellular biology, this would then go and do things, perhaps make proteins ... 

### RNA Sequencing
We take our sample and turn Mature mRNA into RNA fragments, in a process called fragmentation.
Then, with turn those RNA fragments into cDNA fragments, in a process called reverse transcription.
This all goes into a High-throughput sequencing machine to give us sequences/reads of every fragment. This will be an ordered list of the chemicals (A C G T) ?? Do we read the RNA or the c-DNA strand

Major Data Point 1: We now have a count of all the fragments (and what they are)

All of these reads need to get aligned back to the original gene.

### Problems
- Different versions of Mature mRNA (which are referred to as an "isoform") can happen because of an issue during the processing from Pre-mRNA to Mature mRNA including:
  - Alternative transcript initiation
  - Alternative 5' splice site
  - Altertiave 3' splice site
  - Skipped exon
  - Alternative polyadenylation
  - ... others
- It is expensive and takes a long time to do really long reads (e.g., the whole gene), which is why we need to fragment the mRNA into smaller pieces.
  - This means that we might not always capture all the different isoforms.
- Fragments might align to more than one isoform, or to a different gene.

### Goal
**Expression Analysis**, estimate a transcriptome, the set of all expressed transcripts and their frequencies in a cell at a given time. 

- We can measure/calculate the relative expression (i.e., frequency) of two things in our sample:
  - Transcripts (i.e., What proportion of the sample was expressed by a specific isoform, by transcripts?)
    - e.g., There are a total of 100 transcripts in the whole sample. Isoform AAA1 was expressed by 88 of them.
  - Nucleotides (i.e., What proportion of the sample was expressed by a specific isoform, by nucleotides?)
    - e.g., There are a total of 1,000,000 nucleotides in the whole sample. Isoform BBB2 was expressed by 500,000 of them.

### Somethingsomething

Definitions
All definitions are framed for cBioPortal, the software system (which includes a website, and database) for interactive exploration of multidimensional cancer genomics data sets.

TCGA
: The Cancer Genome Atlas, a project that profiles specific cancer types to understand their individual molecular landscape.

PanCanAtlas
: a Pan-Cancer Initiative that took all 33 TCGA cancer types and analyzed them together to compare molecular similarities and differences.

Study
: Often identified by its acronym (e.g., TCGA-LGG) represents the specific tumour cohort that was analyzed.

Ontology
: a set of concepts and categories in a subject area or domain that shows their properties and the relations between them

OncoTree
: the cancer type ontology used by cBioPortal, which has has a root node of Tissue, so Level 1 would be all the Tissue Sites, then they are categorized by cancer types.

cBioPortal Data Model

Database
Studies
Clinical Data
Samples
Sample Lists
Patients
Namespace Attributes
Mutations
Molecular Data
Gene Panel Data
Discrete Copy Number Alterations
Molecular Profiles
Genes
Generic Assays
Generic Assay Data
Gene Panels
Copy Number Segments
Clinical Attributes
Cancer Types




Database, the 
Dataset, the set of files for a particular study
Data Type: Clinical
Data Type: Molecular
Sequencing
RNA Sequencing
RSEM


RSEM defines the following; 

For isoform $i$, 
$\nu_i$, the fraction of nucleotides made up by a given gene or isoform
$$
\nu_i = \frac{\tau_i\ell_i}{\sum_j\tau_j\ell_j}
$$

$\tau_i$, the fraction of transcripts made up by a given gene or isoform
$$
\tau_i = \frac{\nu_i}{\ell_i}\left(\sum_j\frac{\nu_j}{\ell_j}\right)^{-1}
$$

where $\ell_i$ is the length, in nucleotides, of isoform $i$. 

 
The probability that the read sequence
$$
P(r|\theta) = \prod_{n=1}^N\sum_{i=0}^M\theta_i P(r_n|G_n=i)
$$





EM Algorithm
- An iterative method to find (local) maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. 

RSEM Model
- Generates reads from a combination of isoform, start position, and orientation
- $N$ the number of reads generated (independent and identically distributed)
  - i.i.d is an assumption that is supported by uniform sampling, massive parallelization and high depth, random fragmentation, poisson/negative binomial approximation accounting for dispersion
  - i.i.d. for rna-seq is rarely perfect, but is helpful for when we do differential expression analysis
- $L$ the length of reads
- Random Variables
  - $R_n$, read sequences (i.e., observed data)
- Latent Random Variables
  - $G_n$, isoform (from which the read came from)
  - $S_n$, start position (from which the read came from)
  - $O_n$, orientation (from which the read came from)
- Parameters
  - $\theta = [\theta_0, ... , \theta_M]$, the expression levels
    - We assume $M$, all possible isoforms in a transcriptome, is given


More Details
- $G_n \in \{0, ..., M\}$, where $0$ represents a 'noise' isoform, which generates reads that do not map to known isoforms.
  - The probability of any given isoform is determined when the expression levels for that isoform equals 1. 
  - $P(G_n=i | \theta) = \theta_i \text{  where  } \sum_i\theta_i = 1$
- $S_n \in \{1, ..., \text{max}_i\ell_i}$, where $\ell_i$ is the length of isoform $i$
  - $P(S_{n=j} | G_{n=i})$ is called the *read start position distribution* (RPSD)
  - $P(S_{n=j} | G_{n=i}) = \ell^{-1}_i$, for reads generated uniformly across transcripts and mRNA with poly(A) tails
    - The other models are not shown, because the important part is the relationship with length
- $O_n \in \{0, 1\}$
  - 0 indicating that the sequence of read n is in the same orientation as that of its parent isoform
  - 1 indicating thet the sequence of read n is the reverse complement of its parent isoform
- $R_n$ summarizes the hidden random variables for the $n$-th read with a set of indicator random variables
  - $Z_{nijk} \text{  where  } Z_{njk} = 1 \text{ if } (G_n, S_n, O_n) = (i, j, k)$
  - Therefore, the conditional probability of a read sequence derived from an isoform (other than the noise isoform) is given by

$$
P(R_n = \rho | Z_{njk} = 1) = \begin{cases}
    \prod_{t=1}^L w_t(\rho_t , \gamma_{j+t-1}^i) \qquad k=0 \\
    \prod_{t=1}^L w_t(\rho_t , \bar{\gamma}_{j+t-1}^i) \qquad k=1
\end{cases}
$$

  - $\gamma^i$ is the sequence of the isoform $i$ and $\bar{\gamma}^i$ is its reverse complement
  - $w_t(a,b)$ is a position-specific substitution matrix, where the value is the probability that we observe character $a$ as position $t$ of the read given that the corresponding character in the reference isoform sequence is $b$









Expresseions at the gene level is simply the sum of the expression of possible isoforms. 

NOTE: There is some really interesting Maths regarding the RSEM algorithm alternating between the E-step and the M-step that results in the concavity of the log likelihood function (i.e., guaranteed to reach a global maximum). 


What the bloody hell does this even mean?

We're looking at "genes.normalized_results" so it's going to be the sum of isoforms

- RNASeqV2 = Processed using RSEM (RNA-Seq by Expectation-Maximization)
  - Specifically data_mrna_seq_v2_rsem.txt = to the rsem.genes.normalized_results file from TCGA
- RSEM calculates the estimated number of reads that aligned to a transcript (i.e., relative expression)
  - **fraction of transcripts**
  - **fraction of nucleotides**

What do the numbers in data_mrna_seq_v2_rsem.txt mean? 
```{python}
1
```


## Data Cleaning

```{ojs}
//| label: section-div-04
//| column: screen
//| echo: False

html`
<div class="section-div">
  <div>Setup</div>
  <div>Data Acquisition</div>
  <div>Data Understanding</div>
  <div class="current">Data Cleaning</div>
  <div>Dimension Reduction</div>
  <div>Statistical Modelling</div>
  <div>Conclusions</div>
</div>
`
 
```



## Dimension Reduction

```{ojs}
//| label: section-div-05
//| column: screen
//| echo: False

html`
<div class="section-div">
  <div">Setup</div>
  <div>Data Acquisition</div>
  <div>Data Understanding</div>
  <div>Data Cleaning</div>
  <div class="current">Dimension Reduction</div>
  <div>Statistical Modelling</div>
  <div>Conclusions</div>
</div>
`
 
```




## Statistical Modelling

```{ojs}
//| label: section-div-06
//| column: screen
//| echo: False

html`
<div class="section-div">
  <div>Setup</div>
  <div>Data Acquisition</div>
  <div>Data Understanding</div>
  <div>Data Cleaning</div>
  <div>Dimension Reduction</div>
  <div class="current">Statistical Modelling</div>
  <div>Conclusions</div>
</div>
`
 
```

## Conclusions

```{ojs}
//| label: section-div-07
//| column: screen
//| echo: False

html`
<div class="section-div">
  <div>Setup</div>
  <div>Data Acquisition</div>
  <div>Data Understanding</div>
  <div>Data Cleaning</div>
  <div>Dimension Reduction</div>
  <div>Statistical Modelling</div>
  <div class="current">Conclusions</div>
</div>
`
 
```





üß¨ From Shredded Spells to Gene Expression
==========================================

What We Actually Measured
-------------------------

### üîç The Crime Scene (Biological Reality)

-   Tumor tissue contains RNA.

-   RNA represents **which genes are actively being used**.

-   We sequence millions of short fragments (reads).

-   Each read is a tiny piece of a transcript.

* * * * *

üìñ The Reconstruction
---------------------

We assume all fragments came from a known reference "book" (the human transcriptome).

Using **RSEM (RNA-Seq by Expectation Maximization)**:

-   Reads are probabilistically assigned to transcript isoforms.

-   Isoform estimates are aggregated to gene-level estimates.

-   Values are corrected for:

    -   Gene length

    -   Sequencing depth

    -   Multi-mapping reads

The result:

> A gene-by-sample matrix of estimated transcript abundance.

* * * * *

üìä What Each Row Represents
---------------------------

**Hugo_Symbol**\
The official gene name (e.g., TP53, IDH1).

**Entrez_Gene_Id**\
Stable numeric gene identifier (database key).

Each row = one gene.\
Each column = one tumor sample.\
Each value = estimated mRNA abundance for that gene in that tumor.

* * * * *

üìà What the Numbers Mean
------------------------

-   Continuous

-   Non-negative

-   Linear RSEM abundance estimates

-   Proportional to transcript abundance

-   Not raw molecule counts

-   Not mutation data

-   Not protein levels

If Gene A = 200 and Gene B = 100 (within a sample):

‚Üí Gene A has approximately twice the estimated transcript abundance of Gene B.

But:

Magnitude ‚â† importance.

* * * * *

üß† What Matters Most
--------------------

### 1Ô∏è‚É£ Same Gene Across Samples

Biologically meaningful comparison.

### 2Ô∏è‚É£ Variability Across Tumors

High variance genes ‚Üí informative\
Low variance genes ‚Üí housekeeping / background

### 3Ô∏è‚É£ Expression ‚â† Genome

We measured gene **expression**, not gene presence or mutation.

* * * * *

üß¨ Key Vocabulary
-----------------

-   **Gene** -- DNA locus that produces RNA.

-   **Isoform** -- Alternative transcript version of a gene.

-   **Transcriptome** -- Total RNA expression profile of a sample.

-   **RSEM** -- Probabilistic transcript quantification method.

-   **Expected Counts** -- Model-based abundance estimates.

-   **Normalization** -- Adjusting for technical bias (not biological baseline).

-   **Variance** -- Spread across samples; often more informative than raw magnitude.

* * * * *

üé≠ The Big Insight
------------------

Each tumor is not just a disease.

It is:

> A 20,531-dimensional expression profile\
> describing which biological programs are active.

We reconstructed the "spellbook."

But the real story lives in:

-   Which chapters change.

-   Which remain constant.

-   And how patterns emerge across tumors.

* * * * *

‚ö†Ô∏è The Important Limitation
---------------------------

Expression data tells us:

-   What instructions are active.

It does NOT tell us:

-   Why they're active.

-   Whether they're mutated.

-   Whether protein levels match.

-   Whether the change is causal.

This is one layer of biology.

* * * * *

üî• Final Line for Class
-----------------------

> "We didn't count genes.\
> We counted how loudly each gene was being read.\
> And in that noise... the pattern begins to whisper."