[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomics Data Mining Notebook",
    "section": "",
    "text": "html`\n&lt;div class=\"section-div\"&gt;\n  &lt;div class=\"current\"&gt;Setup&lt;/div&gt;\n  &lt;div&gt;Data Acquisition&lt;/div&gt;\n  &lt;div&gt;Data Understanding&lt;/div&gt;\n  &lt;div&gt;Data Cleaning&lt;/div&gt;\n  &lt;div&gt;Dimension Reduction&lt;/div&gt;\n  &lt;div&gt;Statistical Modelling&lt;/div&gt;\n  &lt;div&gt;Conclusions&lt;/div&gt;\n&lt;/div&gt;\n`\n\n\n\n\n\n\nEstablishing a reproducible analysis environment.\n\n# Used if needed to debut environment issues\n\nimport sys, os, site\nprint(\"exe:\", sys.executable)\nprint(\"ver:\", sys.version)\nprint(\"prefix:\", sys.prefix)\nprint(\"base_prefix:\", sys.base_prefix)\nprint(\"cwd:\", os.getcwd())\nprint(\"usersite:\", site.getusersitepackages())\n\n\nfor k in [\"PYTHONHOME\", \"PYTHONPATH\", \"CONDA_PREFIX\", \"VIRTUAL_ENV\"]:\n    print(k, \"=\", os.environ.get(k))\n\n\n\n\n# --- Standard library ---\nimport importlib.metadata\nimport importlib\nimport json\nimport os\nimport platform\nimport random\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport tarfile\nfrom pathlib import Path\nfrom urllib.parse import urlsplit\n\n# --- Third-party ---\nimport itables\nimport numpy as np\nimport pandas as pd\nimport requests\nimport sklearn\nimport yaml\nfrom IPython.display import Markdown, display\nfrom ydata_profiling import ProfileReport\n\n# --- Currently Testing ---\nimport time\nimport cbioportal\nfrom cbioportal.rest import ApiException\nfrom pprint import pprint\n\n# --- Local / project ---\nimport config\n\nimportlib.reload(config)\n\n\n\n\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict\n\n@dataclass\nclass CancerStudy:\n    # Core identity\n    study_id: str\n    study_name: str\n    description: Optional[str]\n\n    # Cancer classification\n    cancer_type_id: Optional[str]\n    cancer_type_name: Optional[str]\n\n    # Clinical summary\n    sample_count: Optional[int] = None\n    patient_count: Optional[int] = None\n\n    # Data modalities present\n    data_types: List[str] = field(default_factory=list)\n\n    # Provenance\n    reference: Optional[str] = None\n    citation: Optional[str] = None\n    pmid: Optional[str] = None\n\n    # Internal / system metadata\n    source_url: Optional[str] = None\n    raw_meta: Dict[str, str] = field(default_factory=dict)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMost parameters are defined in the config.py file, and will eventually be part of environment variables\n\n\n\n# Parameters\nRANDOM_SEED = 3660\n\n\n\n\n\n# ITables\n# itables.init_notebook_mode()\n\n# Pandas\npd.set_option(\"display.max_rows\", 30)\npd.set_option(\"display.max_columns\", 50)\npd.set_option(\"display.width\", 120)\npd.set_option(\"display.max_colwidth\", 60)\n\n# Randomness\nos.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n# Note for later:\n# - use random_state=RANDOM_SEED in sklearn models\n# - TSNE(random_state=RANDOM_SEED)\n# - train_test_split(..., random_state=RANDOM_SEED)\n\n# Others \n# ...\n\n\n\n\n\ndef download_and_extract_studies(*study_ids: str) -&gt; list[Path]:\n    return [_download_and_extract_study(study) for study in study_ids]\n\ndef _download_and_extract_study(study_id: str):\n    url = f\"{config.DATAHUB_BASE}/{study_id}.tar.gz\"\n    dataset_file = download_dataset(url)\n    dataset_path = extract_dataset(dataset_file)\n    return dataset_path\n\ndef download_dataset(url, /, *, folder: Path = config.DEFAULT_DATASET_DOWNLOAD_FOLDER, force: bool = False) -&gt; Path:\n    folder.mkdir(parents=True, exist_ok=True)\n    \n    filename = Path(urlsplit(url).path).name\n    target = folder / filename\n\n    if force or not target.exists():\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        \n        with target.open(\"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        \n    return target\n\ndef extract_dataset(file: Path, /, *, folder: Path = config.DEFAULT_DATASET_EXTRACT_FOLDER, force: bool = False) -&gt; Path:\n    base_name = file.name.removesuffix(\".tar.gz\")\n    extract_path = folder / base_name\n    \n    folder.mkdir(parents=True, exist_ok=True)\n\n    if force or not extract_path.exists():\n        with tarfile.open(file, \"r:gz\") as tar:\n            tar.extractall(folder)\n\n    return extract_path\n\n\n\n\nChecks\n\n# ---------- helpers ----------\n\ndef _run(cmd: list[str]) -&gt; tuple[int, str, str]:\n    p = subprocess.run(cmd, capture_output=True, text=True)\n    return p.returncode, p.stdout, p.stderr\n\ndef _canonical(name: str) -&gt; str:\n    return re.sub(r\"[-_.]+\", \"-\", name).lower().strip()\n\ndef read_environment_yml(path: str | Path = \"environment.yml\"):\n    \"\"\"\n    Parse environment.yml into:\n      - conda_chosen: conda deps declared in environment.yml (name -&gt; version/constraint/None)\n      - pip_chosen: pip deps declared under `- pip:` in environment.yml (name -&gt; version/constraint/None)\n    Notes:\n      - Handles conda pins like `name`, `name=ver`, `name=ver=build`\n      - Handles constraints like `python&gt;=3.11`, `numpy&lt;2`\n      - Normalizes accidental leading '=' in version strings (e.g., '=1.2.3')\n    \"\"\"\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n\n    conda_chosen: dict[str, str | None] = {}\n    pip_chosen: dict[str, str | None] = {}\n\n    deps = data.get(\"dependencies\", []) or []\n    for dep in deps:\n        if isinstance(dep, str):\n            s = dep.strip()\n            if not s:\n                continue\n\n            # constraint case: python&gt;=3.11, numpy&lt;2, etc.\n            m = re.match(r\"^([A-Za-z0-9_.-]+)\\s*([&lt;&gt;=!~].+)$\", s)\n            if m:\n                conda_chosen[_canonical(m.group(1))] = m.group(2).strip()\n                continue\n\n            # conda pins can be: name, name=ver, name=ver=build\n            parts = s.split(\"=\")\n            name = _canonical(parts[0])\n            ver = parts[1] if len(parts) &gt;= 2 else None\n            if ver is not None:\n                ver = ver.strip()\n                if ver.startswith(\"=\") and not ver.startswith(\"==\"):\n                    ver = ver.lstrip(\"=\")\n            conda_chosen[name] = ver\n\n        elif isinstance(dep, dict) and \"pip\" in dep:\n            for p in dep.get(\"pip\", []) or []:\n                ps = str(p).strip()\n                if not ps:\n                    continue\n                if \"==\" in ps:\n                    pkg, ver = ps.split(\"==\", 1)\n                    pip_chosen[_canonical(pkg)] = ver.strip()\n                else:\n                    # store non-exact constraint as-is (&gt;=, &lt;=, ~=, etc.)\n                    pkg = re.split(r\"[&lt;&gt;=!~]\", ps, maxsplit=1)[0].strip()\n                    constraint = ps[len(pkg):].strip() or None\n                    pip_chosen[_canonical(pkg)] = constraint\n\n    return data, conda_chosen, pip_chosen\n\n\ndef _conda_cmd() -&gt; list[str] | None:\n    \"\"\"\n    Find conda.exe even when it's not on PATH (common for notebooks on Windows).\n\n    Priority:\n      1) CONDA_EXE env var (if set)\n      2) PATH lookup (shutil.which(\"conda\"))\n      3) Derive from CONDA_PREFIX (Windows Anaconda layout):\n         CONDA_PREFIX = ...\\\\anaconda3\\\\envs\\\\&lt;env&gt;\n         conda.exe    = ...\\\\anaconda3\\\\Scripts\\\\conda.exe\n    \"\"\"\n    # 1) explicit env var (often not set in notebooks)\n    conda_exe = os.getenv(\"CONDA_EXE\")\n    if conda_exe and Path(conda_exe).exists():\n        return [conda_exe]\n\n    # 2) PATH lookup\n    which = shutil.which(\"conda\")\n    if which:\n        return [which]\n\n    # 3) derive from CONDA_PREFIX on Windows\n    conda_prefix = os.getenv(\"CONDA_PREFIX\")\n    if conda_prefix:\n        p = Path(conda_prefix)\n        # Typical: .../anaconda3/envs/&lt;env&gt; -&gt; base is parent of \"envs\"\n        if p.parent.name.lower() == \"envs\":\n            base = p.parents[1]\n        else:\n            # Sometimes CONDA_PREFIX might be the base env itself\n            base = p\n        candidate = base / \"Scripts\" / \"conda.exe\"\n        if candidate.exists():\n            return [str(candidate)]\n\n    return None\n\n\ndef conda_available() -&gt; bool:\n    cmd = _conda_cmd()\n    if not cmd:\n        return False\n    try:\n        subprocess.run(cmd + [\"--version\"], capture_output=True, text=True, check=True)\n        return True\n    except Exception:\n        return False\n\n\ndef get_conda_installed_versions_strict() -&gt; dict[str, str]:\n    \"\"\"\n    Returns name-&gt;version from `conda list --json`.\n\n    Raises a helpful error if conda cannot be found or if the command fails.\n    \"\"\"\n    cmd = _conda_cmd()\n    if not cmd:\n        raise FileNotFoundError(\n            \"Could not find conda executable (CONDA_EXE not set, conda not on PATH, \"\n            \"and could not derive conda.exe from CONDA_PREFIX).\"\n        )\n\n    proc = subprocess.run(cmd + [\"list\", \"--json\"], capture_output=True, text=True)\n    if proc.returncode != 0:\n        raise RuntimeError(f\"conda list failed:\\nSTDOUT:\\n{proc.stdout}\\nSTDERR:\\n{proc.stderr}\")\n\n    items = json.loads(proc.stdout)\n    out: dict[str, str] = {}\n    for it in items:\n        name = (it.get(\"name\") or \"\").strip().lower()\n        ver = (it.get(\"version\") or \"\").strip()\n        if name and ver:\n            out[name] = ver\n\n    # reflect the running interpreter's python version (helps with \"custom interpreter\" reality)\n    out[\"python\"] = sys.version.split()[0]\n    return out\n\n\ndef get_conda_chosen_from_history() -&gt; dict[str, str | None]:\n    \"\"\"\n    What YOU asked conda for (closest proxy for 'every install command I sent'):\n    `conda env export --from-history`\n    \"\"\"\n    cmd = _conda_cmd()\n    if not cmd:\n        return {}\n\n    proc = subprocess.run(cmd + [\"env\", \"export\", \"--from-history\"], capture_output=True, text=True, check=True)\n    data = yaml.safe_load(proc.stdout) or {}\n    deps = data.get(\"dependencies\", []) or []\n\n    chosen: dict[str, str | None] = {}\n    for dep in deps:\n        if isinstance(dep, str):\n            s = dep.strip()\n            if not s:\n                continue\n            parts = s.split(\"=\")\n            name = parts[0].strip().lower()\n            ver = parts[1].strip().lstrip(\"=\") if len(parts) &gt;= 2 else None\n            chosen[name] = ver\n    return chosen\n\n\ndef get_pip_installed_all() -&gt; dict[str, str]:\n    \"\"\"\n    pip list as json; best effort even inside conda env.\n    \"\"\"\n    rc, out, err = _run([sys.executable, \"-m\", \"pip\", \"list\", \"--format=json\"])\n    if rc != 0:\n        return {}\n    items = json.loads(out)\n    return {_canonical(it[\"name\"]): it[\"version\"] for it in items}\n\n\ndef version_status(expected: str | None, installed: str | None) -&gt; str:\n    if installed is None:\n        return \"MISSING\"\n    if expected is None:\n        return \"OK (un-pinned)\"\n\n    expected = expected.strip()\n\n    # Treat \"=1.2.3\" as an exact pin (common accidental format)\n    if expected.startswith(\"=\") and not expected.startswith(\"==\"):\n        expected = expected.lstrip(\"=\")\n\n    # True constraints (&gt;=, &lt;=, ==, ~=, etc.)\n    if expected and expected[0] in \"&lt;&gt;!~\" or expected.startswith(\"==\"):\n        return \"CHECK (constraint)\"\n\n    if expected.endswith(\".*\"):\n        return \"OK\" if installed.startswith(expected[:-1]) else \"MISMATCH\"\n\n    return \"OK\" if installed == expected else \"MISMATCH\"\n\n\n\n# ---------- run ----------\n\nenv_data, yml_conda_chosen, yml_pip_chosen = read_environment_yml(\"environment.yml\")\n\n# Capture interpreter details (your \"custom interpreter\" concern)\ninterpreter_rows = [\n    {\"Key\": \"sys.executable\", \"Value\": sys.executable},\n    {\"Key\": \"sys.version\", \"Value\": sys.version},\n]\nif os.getenv(\"CONDA_PREFIX\"):\n    interpreter_rows.append({\"Key\": \"CONDA_PREFIX\", \"Value\": os.getenv(\"CONDA_PREFIX\")})\ndf_interpreter = pd.DataFrame(interpreter_rows)\ndisplay(df_interpreter)\n\n# Get actuals\ntry:\n    conda_installed = get_conda_installed_versions_strict() if conda_available() else {}\nexcept Exception as e:\n    print(\"‚ö†Ô∏è Could not read conda installed packages:\", e)\n    conda_installed = {}\n\nconda_history_chosen = get_conda_chosen_from_history() if conda_available() else {}\npip_installed = get_pip_installed_all()\n\nrows: list[dict] = []\n\ndef add_manager_rows(\n    manager: str,\n    chosen_map: dict[str, str | None],\n    installed_versions: dict[str, str],\n):\n    chosen_set = set(chosen_map.keys())\n\n    # chosen packages (source of truth = environment.yml)\n    for pkg in sorted(chosen_map.keys()):\n        exp = chosen_map[pkg]\n        got = installed_versions.get(pkg)\n        status = version_status(exp, got)\n        rows.append({\n            \"Manager\": manager,\n            \"Package\": pkg,\n            \"Chosen?\": True,\n            \"Expected (environment.yml)\": exp if exp is not None else \"(un-pinned)\",\n            \"Installed\": got if got is not None else \"(not installed)\",\n            \"Status\": status,\n        })\n\n    # dependencies (installed but not chosen)\n    for pkg in sorted(set(installed_versions.keys()) - chosen_set):\n        got = installed_versions.get(pkg)\n        rows.append({\n            \"Manager\": manager,\n            \"Package\": pkg,\n            \"Chosen?\": False,\n            \"Expected (environment.yml)\": \"(dependency)\",\n            \"Installed\": got,\n            \"Status\": \"INFO (dependency)\",\n        })\n\n# Conda & Pip rows\nadd_manager_rows(\"conda\", yml_conda_chosen, conda_installed)\nadd_manager_rows(\"pip\", yml_pip_chosen, pip_installed)\n\ndf = pd.DataFrame(rows)\n\n# Helpful sorting: chosen first, then manager, then package\ndf = df.sort_values(by=[\"Chosen?\", \"Manager\", \"Package\"], ascending=[False, True, True]).reset_index(drop=True)\ndisplay(df)\n\n# Summary counts for quick signal\nsummary = df.groupby([\"Manager\", \"Status\"]).size().reset_index(name=\"Count\")\ndisplay(summary)\n\n# Extra: show \"explicit conda-history packages missing from environment.yml\"\nextra_explicit = []\nif conda_history_chosen:\n    for pkg in sorted(set(conda_history_chosen.keys()) - set(yml_conda_chosen.keys())):\n        extra_explicit.append({\n            \"Package\": pkg,\n            \"From conda history\": conda_history_chosen[pkg],\n            \"In environment.yml?\": \"NO\",\n        })\n\ndf_extra_explicit = pd.DataFrame(extra_explicit)\nif not df_extra_explicit.empty:\n    print(\"‚ö†Ô∏è Packages explicitly requested in this conda env (history) but missing from environment.yml:\")\n    display(df_extra_explicit)\nelse:\n    print(\"‚úÖ No extra explicit conda-history packages missing from environment.yml (or conda history unavailable).\")\n\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\n0\nsys.executable\nC:\\dev\\bijan-projects\\genomics-data-mining\\env\\python.exe\n\n\n1\nsys.version\n3.11.14 | packaged by conda-forge | (main, Jan 26 2026, ...\n\n\n2\nCONDA_PREFIX\nC:\\dev\\bijan-projects\\genomics-data-mining\\env\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManager\nPackage\nChosen?\nExpected (environment.yml)\nInstalled\nStatus\n\n\n\n\n0\nconda\ncbioportal\nTrue\n(un-pinned)\n1.0.0\nOK (un-pinned)\n\n\n1\nconda\nipykernel\nTrue\n(un-pinned)\n7.2.0\nOK (un-pinned)\n\n\n2\nconda\nipywidgets\nTrue\n(un-pinned)\n8.1.8\nOK (un-pinned)\n\n\n3\nconda\nitables\nTrue\n(un-pinned)\n2.7.0\nOK (un-pinned)\n\n\n4\nconda\njupyterlab\nTrue\n(un-pinned)\n4.5.4\nOK (un-pinned)\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n399\npip\nwidgetsnbextension\nFalse\n(dependency)\n4.0.15\nINFO (dependency)\n\n\n400\npip\nwin-inet-pton\nFalse\n(dependency)\n1.1.0\nINFO (dependency)\n\n\n401\npip\nwordcloud\nFalse\n(dependency)\n1.9.6\nINFO (dependency)\n\n\n402\npip\nydata-profiling\nFalse\n(dependency)\n0.0.dev0\nINFO (dependency)\n\n\n403\npip\nzipp\nFalse\n(dependency)\n3.23.0\nINFO (dependency)\n\n\n\n\n404 rows √ó 6 columns\n\n\n\n\n\n\n\n\n\n\nManager\nStatus\nCount\n\n\n\n\n0\nconda\nCHECK (constraint)\n1\n\n\n1\nconda\nINFO (dependency)\n227\n\n\n2\nconda\nOK (un-pinned)\n16\n\n\n3\npip\nINFO (dependency)\n160\n\n\n\n\n\n\n\n‚ö†Ô∏è Packages explicitly requested in this conda env (history) but missing from environment.yml:\n\n\n\n\n\n\n\n\n\nPackage\nFrom conda history\nIn environment.yml?\n\n\n\n\n0\nbravado\nNone\nNO\n\n\n1\nsetuptools[version\n'&lt;\nNO"
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Genomics Data Mining Notebook",
    "section": "",
    "text": "html`\n&lt;div class=\"section-div\"&gt;\n  &lt;div class=\"current\"&gt;Setup&lt;/div&gt;\n  &lt;div&gt;Data Acquisition&lt;/div&gt;\n  &lt;div&gt;Data Understanding&lt;/div&gt;\n  &lt;div&gt;Data Cleaning&lt;/div&gt;\n  &lt;div&gt;Dimension Reduction&lt;/div&gt;\n  &lt;div&gt;Statistical Modelling&lt;/div&gt;\n  &lt;div&gt;Conclusions&lt;/div&gt;\n&lt;/div&gt;\n`\n\n\n\n\n\n\nEstablishing a reproducible analysis environment.\n\n# Used if needed to debut environment issues\n\nimport sys, os, site\nprint(\"exe:\", sys.executable)\nprint(\"ver:\", sys.version)\nprint(\"prefix:\", sys.prefix)\nprint(\"base_prefix:\", sys.base_prefix)\nprint(\"cwd:\", os.getcwd())\nprint(\"usersite:\", site.getusersitepackages())\n\n\nfor k in [\"PYTHONHOME\", \"PYTHONPATH\", \"CONDA_PREFIX\", \"VIRTUAL_ENV\"]:\n    print(k, \"=\", os.environ.get(k))\n\n\n\n\n# --- Standard library ---\nimport importlib.metadata\nimport importlib\nimport json\nimport os\nimport platform\nimport random\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport tarfile\nfrom pathlib import Path\nfrom urllib.parse import urlsplit\n\n# --- Third-party ---\nimport itables\nimport numpy as np\nimport pandas as pd\nimport requests\nimport sklearn\nimport yaml\nfrom IPython.display import Markdown, display\nfrom ydata_profiling import ProfileReport\n\n# --- Currently Testing ---\nimport time\nimport cbioportal\nfrom cbioportal.rest import ApiException\nfrom pprint import pprint\n\n# --- Local / project ---\nimport config\n\nimportlib.reload(config)\n\n\n\n\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict\n\n@dataclass\nclass CancerStudy:\n    # Core identity\n    study_id: str\n    study_name: str\n    description: Optional[str]\n\n    # Cancer classification\n    cancer_type_id: Optional[str]\n    cancer_type_name: Optional[str]\n\n    # Clinical summary\n    sample_count: Optional[int] = None\n    patient_count: Optional[int] = None\n\n    # Data modalities present\n    data_types: List[str] = field(default_factory=list)\n\n    # Provenance\n    reference: Optional[str] = None\n    citation: Optional[str] = None\n    pmid: Optional[str] = None\n\n    # Internal / system metadata\n    source_url: Optional[str] = None\n    raw_meta: Dict[str, str] = field(default_factory=dict)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMost parameters are defined in the config.py file, and will eventually be part of environment variables\n\n\n\n# Parameters\nRANDOM_SEED = 3660\n\n\n\n\n\n# ITables\n# itables.init_notebook_mode()\n\n# Pandas\npd.set_option(\"display.max_rows\", 30)\npd.set_option(\"display.max_columns\", 50)\npd.set_option(\"display.width\", 120)\npd.set_option(\"display.max_colwidth\", 60)\n\n# Randomness\nos.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n# Note for later:\n# - use random_state=RANDOM_SEED in sklearn models\n# - TSNE(random_state=RANDOM_SEED)\n# - train_test_split(..., random_state=RANDOM_SEED)\n\n# Others \n# ...\n\n\n\n\n\ndef download_and_extract_studies(*study_ids: str) -&gt; list[Path]:\n    return [_download_and_extract_study(study) for study in study_ids]\n\ndef _download_and_extract_study(study_id: str):\n    url = f\"{config.DATAHUB_BASE}/{study_id}.tar.gz\"\n    dataset_file = download_dataset(url)\n    dataset_path = extract_dataset(dataset_file)\n    return dataset_path\n\ndef download_dataset(url, /, *, folder: Path = config.DEFAULT_DATASET_DOWNLOAD_FOLDER, force: bool = False) -&gt; Path:\n    folder.mkdir(parents=True, exist_ok=True)\n    \n    filename = Path(urlsplit(url).path).name\n    target = folder / filename\n\n    if force or not target.exists():\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        \n        with target.open(\"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        \n    return target\n\ndef extract_dataset(file: Path, /, *, folder: Path = config.DEFAULT_DATASET_EXTRACT_FOLDER, force: bool = False) -&gt; Path:\n    base_name = file.name.removesuffix(\".tar.gz\")\n    extract_path = folder / base_name\n    \n    folder.mkdir(parents=True, exist_ok=True)\n\n    if force or not extract_path.exists():\n        with tarfile.open(file, \"r:gz\") as tar:\n            tar.extractall(folder)\n\n    return extract_path\n\n\n\n\nChecks\n\n# ---------- helpers ----------\n\ndef _run(cmd: list[str]) -&gt; tuple[int, str, str]:\n    p = subprocess.run(cmd, capture_output=True, text=True)\n    return p.returncode, p.stdout, p.stderr\n\ndef _canonical(name: str) -&gt; str:\n    return re.sub(r\"[-_.]+\", \"-\", name).lower().strip()\n\ndef read_environment_yml(path: str | Path = \"environment.yml\"):\n    \"\"\"\n    Parse environment.yml into:\n      - conda_chosen: conda deps declared in environment.yml (name -&gt; version/constraint/None)\n      - pip_chosen: pip deps declared under `- pip:` in environment.yml (name -&gt; version/constraint/None)\n    Notes:\n      - Handles conda pins like `name`, `name=ver`, `name=ver=build`\n      - Handles constraints like `python&gt;=3.11`, `numpy&lt;2`\n      - Normalizes accidental leading '=' in version strings (e.g., '=1.2.3')\n    \"\"\"\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n\n    conda_chosen: dict[str, str | None] = {}\n    pip_chosen: dict[str, str | None] = {}\n\n    deps = data.get(\"dependencies\", []) or []\n    for dep in deps:\n        if isinstance(dep, str):\n            s = dep.strip()\n            if not s:\n                continue\n\n            # constraint case: python&gt;=3.11, numpy&lt;2, etc.\n            m = re.match(r\"^([A-Za-z0-9_.-]+)\\s*([&lt;&gt;=!~].+)$\", s)\n            if m:\n                conda_chosen[_canonical(m.group(1))] = m.group(2).strip()\n                continue\n\n            # conda pins can be: name, name=ver, name=ver=build\n            parts = s.split(\"=\")\n            name = _canonical(parts[0])\n            ver = parts[1] if len(parts) &gt;= 2 else None\n            if ver is not None:\n                ver = ver.strip()\n                if ver.startswith(\"=\") and not ver.startswith(\"==\"):\n                    ver = ver.lstrip(\"=\")\n            conda_chosen[name] = ver\n\n        elif isinstance(dep, dict) and \"pip\" in dep:\n            for p in dep.get(\"pip\", []) or []:\n                ps = str(p).strip()\n                if not ps:\n                    continue\n                if \"==\" in ps:\n                    pkg, ver = ps.split(\"==\", 1)\n                    pip_chosen[_canonical(pkg)] = ver.strip()\n                else:\n                    # store non-exact constraint as-is (&gt;=, &lt;=, ~=, etc.)\n                    pkg = re.split(r\"[&lt;&gt;=!~]\", ps, maxsplit=1)[0].strip()\n                    constraint = ps[len(pkg):].strip() or None\n                    pip_chosen[_canonical(pkg)] = constraint\n\n    return data, conda_chosen, pip_chosen\n\n\ndef _conda_cmd() -&gt; list[str] | None:\n    \"\"\"\n    Find conda.exe even when it's not on PATH (common for notebooks on Windows).\n\n    Priority:\n      1) CONDA_EXE env var (if set)\n      2) PATH lookup (shutil.which(\"conda\"))\n      3) Derive from CONDA_PREFIX (Windows Anaconda layout):\n         CONDA_PREFIX = ...\\\\anaconda3\\\\envs\\\\&lt;env&gt;\n         conda.exe    = ...\\\\anaconda3\\\\Scripts\\\\conda.exe\n    \"\"\"\n    # 1) explicit env var (often not set in notebooks)\n    conda_exe = os.getenv(\"CONDA_EXE\")\n    if conda_exe and Path(conda_exe).exists():\n        return [conda_exe]\n\n    # 2) PATH lookup\n    which = shutil.which(\"conda\")\n    if which:\n        return [which]\n\n    # 3) derive from CONDA_PREFIX on Windows\n    conda_prefix = os.getenv(\"CONDA_PREFIX\")\n    if conda_prefix:\n        p = Path(conda_prefix)\n        # Typical: .../anaconda3/envs/&lt;env&gt; -&gt; base is parent of \"envs\"\n        if p.parent.name.lower() == \"envs\":\n            base = p.parents[1]\n        else:\n            # Sometimes CONDA_PREFIX might be the base env itself\n            base = p\n        candidate = base / \"Scripts\" / \"conda.exe\"\n        if candidate.exists():\n            return [str(candidate)]\n\n    return None\n\n\ndef conda_available() -&gt; bool:\n    cmd = _conda_cmd()\n    if not cmd:\n        return False\n    try:\n        subprocess.run(cmd + [\"--version\"], capture_output=True, text=True, check=True)\n        return True\n    except Exception:\n        return False\n\n\ndef get_conda_installed_versions_strict() -&gt; dict[str, str]:\n    \"\"\"\n    Returns name-&gt;version from `conda list --json`.\n\n    Raises a helpful error if conda cannot be found or if the command fails.\n    \"\"\"\n    cmd = _conda_cmd()\n    if not cmd:\n        raise FileNotFoundError(\n            \"Could not find conda executable (CONDA_EXE not set, conda not on PATH, \"\n            \"and could not derive conda.exe from CONDA_PREFIX).\"\n        )\n\n    proc = subprocess.run(cmd + [\"list\", \"--json\"], capture_output=True, text=True)\n    if proc.returncode != 0:\n        raise RuntimeError(f\"conda list failed:\\nSTDOUT:\\n{proc.stdout}\\nSTDERR:\\n{proc.stderr}\")\n\n    items = json.loads(proc.stdout)\n    out: dict[str, str] = {}\n    for it in items:\n        name = (it.get(\"name\") or \"\").strip().lower()\n        ver = (it.get(\"version\") or \"\").strip()\n        if name and ver:\n            out[name] = ver\n\n    # reflect the running interpreter's python version (helps with \"custom interpreter\" reality)\n    out[\"python\"] = sys.version.split()[0]\n    return out\n\n\ndef get_conda_chosen_from_history() -&gt; dict[str, str | None]:\n    \"\"\"\n    What YOU asked conda for (closest proxy for 'every install command I sent'):\n    `conda env export --from-history`\n    \"\"\"\n    cmd = _conda_cmd()\n    if not cmd:\n        return {}\n\n    proc = subprocess.run(cmd + [\"env\", \"export\", \"--from-history\"], capture_output=True, text=True, check=True)\n    data = yaml.safe_load(proc.stdout) or {}\n    deps = data.get(\"dependencies\", []) or []\n\n    chosen: dict[str, str | None] = {}\n    for dep in deps:\n        if isinstance(dep, str):\n            s = dep.strip()\n            if not s:\n                continue\n            parts = s.split(\"=\")\n            name = parts[0].strip().lower()\n            ver = parts[1].strip().lstrip(\"=\") if len(parts) &gt;= 2 else None\n            chosen[name] = ver\n    return chosen\n\n\ndef get_pip_installed_all() -&gt; dict[str, str]:\n    \"\"\"\n    pip list as json; best effort even inside conda env.\n    \"\"\"\n    rc, out, err = _run([sys.executable, \"-m\", \"pip\", \"list\", \"--format=json\"])\n    if rc != 0:\n        return {}\n    items = json.loads(out)\n    return {_canonical(it[\"name\"]): it[\"version\"] for it in items}\n\n\ndef version_status(expected: str | None, installed: str | None) -&gt; str:\n    if installed is None:\n        return \"MISSING\"\n    if expected is None:\n        return \"OK (un-pinned)\"\n\n    expected = expected.strip()\n\n    # Treat \"=1.2.3\" as an exact pin (common accidental format)\n    if expected.startswith(\"=\") and not expected.startswith(\"==\"):\n        expected = expected.lstrip(\"=\")\n\n    # True constraints (&gt;=, &lt;=, ==, ~=, etc.)\n    if expected and expected[0] in \"&lt;&gt;!~\" or expected.startswith(\"==\"):\n        return \"CHECK (constraint)\"\n\n    if expected.endswith(\".*\"):\n        return \"OK\" if installed.startswith(expected[:-1]) else \"MISMATCH\"\n\n    return \"OK\" if installed == expected else \"MISMATCH\"\n\n\n\n# ---------- run ----------\n\nenv_data, yml_conda_chosen, yml_pip_chosen = read_environment_yml(\"environment.yml\")\n\n# Capture interpreter details (your \"custom interpreter\" concern)\ninterpreter_rows = [\n    {\"Key\": \"sys.executable\", \"Value\": sys.executable},\n    {\"Key\": \"sys.version\", \"Value\": sys.version},\n]\nif os.getenv(\"CONDA_PREFIX\"):\n    interpreter_rows.append({\"Key\": \"CONDA_PREFIX\", \"Value\": os.getenv(\"CONDA_PREFIX\")})\ndf_interpreter = pd.DataFrame(interpreter_rows)\ndisplay(df_interpreter)\n\n# Get actuals\ntry:\n    conda_installed = get_conda_installed_versions_strict() if conda_available() else {}\nexcept Exception as e:\n    print(\"‚ö†Ô∏è Could not read conda installed packages:\", e)\n    conda_installed = {}\n\nconda_history_chosen = get_conda_chosen_from_history() if conda_available() else {}\npip_installed = get_pip_installed_all()\n\nrows: list[dict] = []\n\ndef add_manager_rows(\n    manager: str,\n    chosen_map: dict[str, str | None],\n    installed_versions: dict[str, str],\n):\n    chosen_set = set(chosen_map.keys())\n\n    # chosen packages (source of truth = environment.yml)\n    for pkg in sorted(chosen_map.keys()):\n        exp = chosen_map[pkg]\n        got = installed_versions.get(pkg)\n        status = version_status(exp, got)\n        rows.append({\n            \"Manager\": manager,\n            \"Package\": pkg,\n            \"Chosen?\": True,\n            \"Expected (environment.yml)\": exp if exp is not None else \"(un-pinned)\",\n            \"Installed\": got if got is not None else \"(not installed)\",\n            \"Status\": status,\n        })\n\n    # dependencies (installed but not chosen)\n    for pkg in sorted(set(installed_versions.keys()) - chosen_set):\n        got = installed_versions.get(pkg)\n        rows.append({\n            \"Manager\": manager,\n            \"Package\": pkg,\n            \"Chosen?\": False,\n            \"Expected (environment.yml)\": \"(dependency)\",\n            \"Installed\": got,\n            \"Status\": \"INFO (dependency)\",\n        })\n\n# Conda & Pip rows\nadd_manager_rows(\"conda\", yml_conda_chosen, conda_installed)\nadd_manager_rows(\"pip\", yml_pip_chosen, pip_installed)\n\ndf = pd.DataFrame(rows)\n\n# Helpful sorting: chosen first, then manager, then package\ndf = df.sort_values(by=[\"Chosen?\", \"Manager\", \"Package\"], ascending=[False, True, True]).reset_index(drop=True)\ndisplay(df)\n\n# Summary counts for quick signal\nsummary = df.groupby([\"Manager\", \"Status\"]).size().reset_index(name=\"Count\")\ndisplay(summary)\n\n# Extra: show \"explicit conda-history packages missing from environment.yml\"\nextra_explicit = []\nif conda_history_chosen:\n    for pkg in sorted(set(conda_history_chosen.keys()) - set(yml_conda_chosen.keys())):\n        extra_explicit.append({\n            \"Package\": pkg,\n            \"From conda history\": conda_history_chosen[pkg],\n            \"In environment.yml?\": \"NO\",\n        })\n\ndf_extra_explicit = pd.DataFrame(extra_explicit)\nif not df_extra_explicit.empty:\n    print(\"‚ö†Ô∏è Packages explicitly requested in this conda env (history) but missing from environment.yml:\")\n    display(df_extra_explicit)\nelse:\n    print(\"‚úÖ No extra explicit conda-history packages missing from environment.yml (or conda history unavailable).\")\n\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\n0\nsys.executable\nC:\\dev\\bijan-projects\\genomics-data-mining\\env\\python.exe\n\n\n1\nsys.version\n3.11.14 | packaged by conda-forge | (main, Jan 26 2026, ...\n\n\n2\nCONDA_PREFIX\nC:\\dev\\bijan-projects\\genomics-data-mining\\env\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManager\nPackage\nChosen?\nExpected (environment.yml)\nInstalled\nStatus\n\n\n\n\n0\nconda\ncbioportal\nTrue\n(un-pinned)\n1.0.0\nOK (un-pinned)\n\n\n1\nconda\nipykernel\nTrue\n(un-pinned)\n7.2.0\nOK (un-pinned)\n\n\n2\nconda\nipywidgets\nTrue\n(un-pinned)\n8.1.8\nOK (un-pinned)\n\n\n3\nconda\nitables\nTrue\n(un-pinned)\n2.7.0\nOK (un-pinned)\n\n\n4\nconda\njupyterlab\nTrue\n(un-pinned)\n4.5.4\nOK (un-pinned)\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n399\npip\nwidgetsnbextension\nFalse\n(dependency)\n4.0.15\nINFO (dependency)\n\n\n400\npip\nwin-inet-pton\nFalse\n(dependency)\n1.1.0\nINFO (dependency)\n\n\n401\npip\nwordcloud\nFalse\n(dependency)\n1.9.6\nINFO (dependency)\n\n\n402\npip\nydata-profiling\nFalse\n(dependency)\n0.0.dev0\nINFO (dependency)\n\n\n403\npip\nzipp\nFalse\n(dependency)\n3.23.0\nINFO (dependency)\n\n\n\n\n404 rows √ó 6 columns\n\n\n\n\n\n\n\n\n\n\nManager\nStatus\nCount\n\n\n\n\n0\nconda\nCHECK (constraint)\n1\n\n\n1\nconda\nINFO (dependency)\n227\n\n\n2\nconda\nOK (un-pinned)\n16\n\n\n3\npip\nINFO (dependency)\n160\n\n\n\n\n\n\n\n‚ö†Ô∏è Packages explicitly requested in this conda env (history) but missing from environment.yml:\n\n\n\n\n\n\n\n\n\nPackage\nFrom conda history\nIn environment.yml?\n\n\n\n\n0\nbravado\nNone\nNO\n\n\n1\nsetuptools[version\n'&lt;\nNO"
  },
  {
    "objectID": "index.html#data-acquisition",
    "href": "index.html#data-acquisition",
    "title": "Genomics Data Mining Notebook",
    "section": "Data Acquisition",
    "text": "Data Acquisition\n\nhtml`\n&lt;div class=\"section-div\"&gt;\n  &lt;div&gt;Setup&lt;/div&gt;\n  &lt;div class=\"current\"&gt;Data Acquisition&lt;/div&gt;\n  &lt;div&gt;Data Understanding&lt;/div&gt;\n  &lt;div&gt;Data Cleaning&lt;/div&gt;\n  &lt;div&gt;Dimension Reduction&lt;/div&gt;\n  &lt;div&gt;Statistical Modelling&lt;/div&gt;\n  &lt;div&gt;Conclusions&lt;/div&gt;\n&lt;/div&gt;\n`\n\n\n\n\n\n\nWe use the cBioPortal to gather data about what is available.\n\ncBioPortal\n\nCancer Types\n\n# Cancer Types API\ncancer_types_api_config = cbioportal.Configuration()\ncancer_types_api_instance = cbioportal.CancerTypesApi(cbioportal.ApiClient(cancer_types_api_config))\n\ncbioportal_cancer_types = cancer_types_api_instance.get_all_cancer_types_using_get()\ncbioportal_cancer_types = pd.DataFrame([item.to_dict() for item in cbioportal_cancer_types])\n\nitables.show(\n    cbioportal_cancer_types\n)\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.7.0 from the internet...\n    (need help?)\n    \n\n\n\n\n\n\nStudies\n\n# Studies API\nstudies_api_instance = cbioportal.StudiesApi()\n\ncbioportal_studies = studies_api_instance.get_all_studies_using_get()\ncbioportal_studies = pd.DataFrame([item.to_dict() for item in cbioportal_studies])\n\nitables.show(\n    cbioportal_studies\n)\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.7.0 from the internet...\n    (need help?)\n    \n\n\n\n\n\n\nClinical Attributes\n\n# Clinical Attributes API\n\nlgg_clinical_attributes = cbioportal.ClinicalAttributesApi().get_all_clinical_attributes_in_study_using_get('lgg_tcga_pan_can_atlas_2018')\nlgg_clinical_attributes = pd.DataFrame([item.to_dict() for item in lgg_clinical_attributes])\n\nitables.show(\n    lgg_clinical_attributes\n)\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.7.0 from the internet...\n    (need help?)\n    \n\n\n\n\n\n\n\nDownload and Extract\nThe Brain Lower Grade Glioma (TCGA, PanCancer Atlas) dataset was obtained from the cBioPortal for Cancer Genomics. The dataset was downloaded as a compressed tarball containing clinical, molecular, and metadata files.\nDataset: Brain Lower Grade Glioma (TCGA, PanCancer Atlas)\nSource: https://www.cbioportal.org/\nFile: lgg_tcga_pan_can_atlas_2018.tar.gz\n\nstudy_paths = download_and_extract_studies(\n    \"lgg_tcga_pan_can_atlas_2018\",\n    \"lgg_ucsf_2014\"\n)\nstudy_paths\n\n[WindowsPath('C:/dev/bijan-projects/genomics-data-mining/data/external/extracted/lgg_tcga_pan_can_atlas_2018'),\n WindowsPath('C:/dev/bijan-projects/genomics-data-mining/data/external/extracted/lgg_ucsf_2014')]\n\n\n\n\n\n\n\n\nNotecBioPortal Downloading\n\n\n\nThere is extensive documentation for how to download from cBioPortal. This includes manually through the browser, or with an API. https://docs.cbioportal.org/downloads/\n\n\n\n\nData Loading\nUsing information from the cBioPortal Summary tab and the downloaded data files, the following provides a high-level overview of the dataset, including patient counts, molecular data availability, mutation frequencies, and survival information.\n\ndef describe_dataframe_table(df: pd.DataFrame) -&gt; pd.DataFrame:\n    rows = []\n\n    for col in df.columns:\n        s = df[col]\n\n        row = {\n            \"Column Name\": col,\n            \"Column Type\": str(s.dtype),\n            \"Non-Null Count\": s.notna().sum(),\n            \"Missing Count\": s.isna().sum(),\n        }\n\n        if pd.api.types.is_numeric_dtype(s):\n            row.update({\n                \"Data Kind\": \"Numeric\",\n                \"Min\": s.min(),\n                \"Max\": s.max(),\n                \"Categories\": None,\n            })\n        else:\n            row.update({\n                \"Data Kind\": \"Categorical\",\n                \"Min\": None,\n                \"Max\": None,\n                \"Categories\": \", \".join(map(str, s.dropna().unique()[:10])),\n            })\n\n        rows.append(row)\n\n    return pd.DataFrame(rows)\n\n\n\nPatientClinical SampleMRNA Seq\n\n\n\nclinical_patient_file = Path(\"data/external/extracted/lgg_tcga_pan_can_atlas_2018/data_clinical_patient.txt\")\n\ndf_clinical_patients = pd.read_csv(clinical_patient_file, sep=\"\\t\", comment=\"#\")\n\ndescribed_df_clinical_patients = describe_dataframe_table(df_clinical_patients)\n\n\nitables.show(\n    described_df_clinical_patients,\n    paging=False,\n    text_in_header_can_be_selected=False,\n    columnControl=[\"order\", \"colVisDropdown\", \"searchDropdown\"],\n    ordering={\"indicators\": False, \"handler\": False}\n)\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.7.0 from the internet...\n    (need help?)\n    \n\n\n\n\n\n\n\nclinical_sample_file = Path(\"data/external/extracted/lgg_tcga_pan_can_atlas_2018/data_clinical_patient.txt\")\n\n\n\ndf_clinical_sample = pd.read_csv(clinical_sample_file, sep=\"\\t\", comment=\"#\")\n\n\n\n\nmrna_seq_file = Path(\"data/external/extracted/lgg_tcga_pan_can_atlas_2018/data_mrna_seq_v2_rsem.txt\")\n\n\ndf_mrna_seq = pd.read_csv(mrna_seq_file, sep=\"\\t\", comment=\"#\")\n\nitables.show(\n    df_mrna_seq\n)\n\n# expr = df.iloc[:, 2:]\n\n# gene_summary = pd.DataFrame({\n#     \"mean\": expr.mean(axis=1),\n#     \"median\": expr.median(axis=1),\n#     \"std\": expr.std(axis=1),\n#     \"variance\": expr.var(axis=1),\n#     \"nonzero_fraction\": (expr &gt; 0).mean(axis=1)\n# })\n\n# gene_summary.head()\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.7.0 from the internet...\n    (need help?)\n    \n\n\n\n\n\n\n\n\n\n\nTODO: data_clinical_sample.txt\n\n\nTODO: Summarize the 3 df‚Äôs\n\n\nTODO:\nThesse are some words in this section"
  },
  {
    "objectID": "index.html#data-understanding",
    "href": "index.html#data-understanding",
    "title": "Genomics Data Mining Notebook",
    "section": "Data Understanding",
    "text": "Data Understanding\n\nhtml`\n&lt;div class=\"section-div\"&gt;\n  &lt;div&gt;Setup&lt;/div&gt;\n  &lt;div&gt;Data Acquisition&lt;/div&gt;\n  &lt;div class=\"current\"&gt;Data Understanding&lt;/div&gt;\n  &lt;div&gt;Data Cleaning&lt;/div&gt;\n  &lt;div&gt;Dimension Reduction&lt;/div&gt;\n  &lt;div&gt;Statistical Modelling&lt;/div&gt;\n  &lt;div&gt;Conclusions&lt;/div&gt;\n&lt;/div&gt;\n`"
  },
  {
    "objectID": "index.html#data-cleaning",
    "href": "index.html#data-cleaning",
    "title": "Genomics Data Mining Notebook",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nhtml`\n&lt;div class=\"section-div\"&gt;\n  &lt;div&gt;Setup&lt;/div&gt;\n  &lt;div&gt;Data Acquisition&lt;/div&gt;\n  &lt;div&gt;Data Understanding&lt;/div&gt;\n  &lt;div class=\"current\"&gt;Data Cleaning&lt;/div&gt;\n  &lt;div&gt;Dimension Reduction&lt;/div&gt;\n  &lt;div&gt;Statistical Modelling&lt;/div&gt;\n  &lt;div&gt;Conclusions&lt;/div&gt;\n&lt;/div&gt;\n`"
  },
  {
    "objectID": "index.html#dimension-reduction",
    "href": "index.html#dimension-reduction",
    "title": "Genomics Data Mining Notebook",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nhtml`\n&lt;div class=\"section-div\"&gt;\n  &lt;div\"&gt;Setup&lt;/div&gt;\n  &lt;div&gt;Data Acquisition&lt;/div&gt;\n  &lt;div&gt;Data Understanding&lt;/div&gt;\n  &lt;div&gt;Data Cleaning&lt;/div&gt;\n  &lt;div class=\"current\"&gt;Dimension Reduction&lt;/div&gt;\n  &lt;div&gt;Statistical Modelling&lt;/div&gt;\n  &lt;div&gt;Conclusions&lt;/div&gt;\n&lt;/div&gt;\n`"
  },
  {
    "objectID": "index.html#statistical-modelling",
    "href": "index.html#statistical-modelling",
    "title": "Genomics Data Mining Notebook",
    "section": "Statistical Modelling",
    "text": "Statistical Modelling\n\nhtml`\n&lt;div class=\"section-div\"&gt;\n  &lt;div&gt;Setup&lt;/div&gt;\n  &lt;div&gt;Data Acquisition&lt;/div&gt;\n  &lt;div&gt;Data Understanding&lt;/div&gt;\n  &lt;div&gt;Data Cleaning&lt;/div&gt;\n  &lt;div&gt;Dimension Reduction&lt;/div&gt;\n  &lt;div class=\"current\"&gt;Statistical Modelling&lt;/div&gt;\n  &lt;div&gt;Conclusions&lt;/div&gt;\n&lt;/div&gt;\n`"
  },
  {
    "objectID": "index.html#conclusions",
    "href": "index.html#conclusions",
    "title": "Genomics Data Mining Notebook",
    "section": "Conclusions",
    "text": "Conclusions\n\nhtml`\n&lt;div class=\"section-div\"&gt;\n  &lt;div&gt;Setup&lt;/div&gt;\n  &lt;div&gt;Data Acquisition&lt;/div&gt;\n  &lt;div&gt;Data Understanding&lt;/div&gt;\n  &lt;div&gt;Data Cleaning&lt;/div&gt;\n  &lt;div&gt;Dimension Reduction&lt;/div&gt;\n  &lt;div&gt;Statistical Modelling&lt;/div&gt;\n  &lt;div class=\"current\"&gt;Conclusions&lt;/div&gt;\n&lt;/div&gt;\n`"
  },
  {
    "objectID": "index.html#what-we-actually-measured",
    "href": "index.html#what-we-actually-measured",
    "title": "Genomics Data Mining Notebook",
    "section": "What We Actually Measured",
    "text": "What We Actually Measured\n\nüîç The Crime Scene (Biological Reality)\n\nTumor tissue contains RNA.\nRNA represents which genes are actively being used.\nWe sequence millions of short fragments (reads).\nEach read is a tiny piece of a transcript."
  },
  {
    "objectID": "index.html#the-reconstruction",
    "href": "index.html#the-reconstruction",
    "title": "Genomics Data Mining Notebook",
    "section": "üìñ The Reconstruction",
    "text": "üìñ The Reconstruction\nWe assume all fragments came from a known reference ‚Äúbook‚Äù (the human transcriptome).\nUsing RSEM (RNA-Seq by Expectation Maximization):\n\nReads are probabilistically assigned to transcript isoforms.\nIsoform estimates are aggregated to gene-level estimates.\nValues are corrected for:\n\nGene length\nSequencing depth\nMulti-mapping reads\n\n\nThe result:\n\nA gene-by-sample matrix of estimated transcript abundance."
  },
  {
    "objectID": "index.html#what-each-row-represents",
    "href": "index.html#what-each-row-represents",
    "title": "Genomics Data Mining Notebook",
    "section": "üìä What Each Row Represents",
    "text": "üìä What Each Row Represents\nHugo_Symbol\nThe official gene name (e.g., TP53, IDH1).\nEntrez_Gene_Id\nStable numeric gene identifier (database key).\nEach row = one gene.\nEach column = one tumor sample.\nEach value = estimated mRNA abundance for that gene in that tumor."
  },
  {
    "objectID": "index.html#what-the-numbers-mean",
    "href": "index.html#what-the-numbers-mean",
    "title": "Genomics Data Mining Notebook",
    "section": "üìà What the Numbers Mean",
    "text": "üìà What the Numbers Mean\n\nContinuous\nNon-negative\nLinear RSEM abundance estimates\nProportional to transcript abundance\nNot raw molecule counts\nNot mutation data\nNot protein levels\n\nIf Gene A = 200 and Gene B = 100 (within a sample):\n‚Üí Gene A has approximately twice the estimated transcript abundance of Gene B.\nBut:\nMagnitude ‚â† importance."
  },
  {
    "objectID": "index.html#what-matters-most",
    "href": "index.html#what-matters-most",
    "title": "Genomics Data Mining Notebook",
    "section": "üß† What Matters Most",
    "text": "üß† What Matters Most\n\n1Ô∏è‚É£ Same Gene Across Samples\nBiologically meaningful comparison.\n\n\n2Ô∏è‚É£ Variability Across Tumors\nHigh variance genes ‚Üí informative\nLow variance genes ‚Üí housekeeping / background\n\n\n3Ô∏è‚É£ Expression ‚â† Genome\nWe measured gene expression, not gene presence or mutation."
  },
  {
    "objectID": "index.html#key-vocabulary",
    "href": "index.html#key-vocabulary",
    "title": "Genomics Data Mining Notebook",
    "section": "üß¨ Key Vocabulary",
    "text": "üß¨ Key Vocabulary\n\nGene ‚Äì DNA locus that produces RNA.\nIsoform ‚Äì Alternative transcript version of a gene.\nTranscriptome ‚Äì Total RNA expression profile of a sample.\nRSEM ‚Äì Probabilistic transcript quantification method.\nExpected Counts ‚Äì Model-based abundance estimates.\nNormalization ‚Äì Adjusting for technical bias (not biological baseline).\nVariance ‚Äì Spread across samples; often more informative than raw magnitude."
  },
  {
    "objectID": "index.html#the-big-insight",
    "href": "index.html#the-big-insight",
    "title": "Genomics Data Mining Notebook",
    "section": "üé≠ The Big Insight",
    "text": "üé≠ The Big Insight\nEach tumor is not just a disease.\nIt is:\n\nA 20,531-dimensional expression profile\ndescribing which biological programs are active.\n\nWe reconstructed the ‚Äúspellbook.‚Äù\nBut the real story lives in:\n\nWhich chapters change.\nWhich remain constant.\nAnd how patterns emerge across tumors."
  },
  {
    "objectID": "index.html#the-important-limitation",
    "href": "index.html#the-important-limitation",
    "title": "Genomics Data Mining Notebook",
    "section": "‚ö†Ô∏è The Important Limitation",
    "text": "‚ö†Ô∏è The Important Limitation\nExpression data tells us:\n\nWhat instructions are active.\n\nIt does NOT tell us:\n\nWhy they‚Äôre active.\nWhether they‚Äôre mutated.\nWhether protein levels match.\nWhether the change is causal.\n\nThis is one layer of biology."
  },
  {
    "objectID": "index.html#final-line-for-class",
    "href": "index.html#final-line-for-class",
    "title": "Genomics Data Mining Notebook",
    "section": "üî• Final Line for Class",
    "text": "üî• Final Line for Class\n\n‚ÄúWe didn‚Äôt count genes.\nWe counted how loudly each gene was being read.\nAnd in that noise‚Ä¶ the pattern begins to whisper.‚Äù"
  }
]